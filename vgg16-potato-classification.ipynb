{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lyxbash/vgg16-potato-classification?scriptVersionId=133763178\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"213a6365","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-06-16T05:58:01.206289Z","iopub.status.busy":"2023-06-16T05:58:01.205686Z","iopub.status.idle":"2023-06-16T05:58:16.356216Z","shell.execute_reply":"2023-06-16T05:58:16.355255Z"},"papermill":{"duration":15.158463,"end_time":"2023-06-16T05:58:16.358868","exception":false,"start_time":"2023-06-16T05:58:01.200405","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"]}],"source":["#Input layer is used to define the shape of the input data for the model.\n","#Lambda layer is used to define custom operations or functions that transform the input data.\n","#Dense layer represents a fully connected layer, where each neuron is connected to every neuron in the previous layer.\n","#Flatten layer is used to flatten the input data into a 1-dimensional array.\n","from keras.layers import Input, Lambda, Dense, Flatten\n","\n","\n","#The Model class is used to create the model architecture by specifying the input and output layers.\n","from keras.models import Model\n","\n","\n","#This line imports the pre-trained VGG16 model from Keras. \n","# VGG16 is a convolutional neural network architecture that has been pre-trained on a large dataset called ImageNet. \n","# Importing it allows you to use the pre-trained weights and architecture for your own tasks.\n","from keras.applications.vgg16 import VGG16\n","\n","\n","# The preprocess_input function is used to preprocess input images according to the requirements of the VGG16 model. \n","# It performs necessary preprocessing steps such as mean subtraction and \n","# scaling to prepare the image data for input to the model.\n","from keras.applications.vgg16 import preprocess_input\n","\n","\n","# The image module provides functions for loading and manipulating images.\n","# It is commonly used for loading images into NumPy arrays and performing various image operations.\n","from keras.preprocessing import image\n","\n","\n","# The ImageDataGenerator class is used for data augmentation and generating batches of image data during training. \n","# It provides various methods to perform data augmentation techniques such as rotation, scaling, \n","# and flipping, which helps to increase the diversity of the training data.\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","\n","# The Sequential class is an alternative way to define a model architecture in Keras. \n","# It allows you to build a model by stacking layers sequentially, one on top of the other.\n","from keras.models import Sequential\n","\n","\n","# The Input layer is used to define the input shape of the model. \n","#It specifies the shape of the input data, such as the dimensions of an image or the length of a sequence.\n","from keras.layers import Input\n","\n","\n","#he glob function is used to retrieve filenames or pathnames that match a specific pattern.\n","#It is often used for file handling and directory operations.\n","from glob import glob\n","\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"id":"1729afc2","metadata":{"execution":{"iopub.execute_input":"2023-06-16T05:58:16.368714Z","iopub.status.busy":"2023-06-16T05:58:16.367495Z","iopub.status.idle":"2023-06-16T05:58:16.373398Z","shell.execute_reply":"2023-06-16T05:58:16.372533Z"},"papermill":{"duration":0.012635,"end_time":"2023-06-16T05:58:16.37549","exception":false,"start_time":"2023-06-16T05:58:16.362855","status":"completed"},"tags":[]},"outputs":[],"source":["input_shape = (224, 224, 3)"]},{"cell_type":"code","execution_count":3,"id":"aa80301c","metadata":{"execution":{"iopub.execute_input":"2023-06-16T05:58:16.383876Z","iopub.status.busy":"2023-06-16T05:58:16.38359Z","iopub.status.idle":"2023-06-16T05:58:22.767301Z","shell.execute_reply":"2023-06-16T05:58:22.766533Z"},"papermill":{"duration":6.411998,"end_time":"2023-06-16T05:58:22.790984","exception":false,"start_time":"2023-06-16T05:58:16.378986","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58889256/58889256 [==============================] - 0s 0us/step\n","Model: \"vgg16\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["#the model will be initialized with pre-trained weights obtained by training on the ImageNet dataset.\n","\n","#The include_top parameter is set to False, which means that the final fully connected layers (classification layers) \n","#of the VGG16 model will not be included. By excluding the top layers, we can add our own custom layers for our specific task.\n","vgg = VGG16(input_shape = input_shape, weights = 'imagenet', include_top = False)\n","vgg.summary()\n","\n","\n","# trainable attribute of each layer to False. By doing this, we freeze the weights of all the layers in the VGG16 model, \n","#preventing them from being updated during training.\n","for layer in vgg.layers:\n","    layer.trainable = True"]},{"cell_type":"code","execution_count":4,"id":"f67a95c9","metadata":{"execution":{"iopub.execute_input":"2023-06-16T05:58:22.807298Z","iopub.status.busy":"2023-06-16T05:58:22.806938Z","iopub.status.idle":"2023-06-16T05:58:22.818063Z","shell.execute_reply":"2023-06-16T05:58:22.817187Z"},"papermill":{"duration":0.021521,"end_time":"2023-06-16T05:58:22.82001","exception":false,"start_time":"2023-06-16T05:58:22.798489","status":"completed"},"tags":[]},"outputs":[],"source":["#glob module is used to retrieve subdirectories within the 'train/' directory.\n","folders = glob('/kaggle/input/potato-disease/train/*')"]},{"cell_type":"code","execution_count":5,"id":"645baf73","metadata":{"execution":{"iopub.execute_input":"2023-06-16T05:58:22.836031Z","iopub.status.busy":"2023-06-16T05:58:22.835669Z","iopub.status.idle":"2023-06-16T05:58:22.864706Z","shell.execute_reply":"2023-06-16T05:58:22.863819Z"},"papermill":{"duration":0.039555,"end_time":"2023-06-16T05:58:22.867087","exception":false,"start_time":"2023-06-16T05:58:22.827532","status":"completed"},"tags":[]},"outputs":[],"source":["#The Flatten layer takes the multidimensional output from the previous layer (which is the output of the VGG16 model) and \n","#converts it into a one-dimensional array. This flattening operation is necessary to \n","#transition from the convolutional layers of VGG16 to the fully connected layers that will follow.\n","\n","#The x variable holds the flattened output, which will serve as input for the next layer.\n","x = Flatten()(vgg.output)\n","\n","\n","#adds a Dense layer on top of the flattened output (x). The Dense layer represents a fully connected layer,\n","#where each neuron is connected to every neuron in the previous layer. \n","#The number of neurons in this layer is set to len(folders), which is the number of classes in your dataset\n","\n","#prediction variable holds the output of this Dense layer, which will be the final prediction of the model.\n","prediction = Dense(len(folders),activation='softmax')(x)\n","\n","\n","model = Model(inputs=vgg.input,outputs=prediction)"]},{"cell_type":"code","execution_count":6,"id":"bc84c8bd","metadata":{"execution":{"iopub.execute_input":"2023-06-16T05:58:22.884575Z","iopub.status.busy":"2023-06-16T05:58:22.884292Z","iopub.status.idle":"2023-06-16T05:58:22.923732Z","shell.execute_reply":"2023-06-16T05:58:22.923078Z"},"papermill":{"duration":0.081533,"end_time":"2023-06-16T05:58:22.95668","exception":false,"start_time":"2023-06-16T05:58:22.875147","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 25088)             0         \n","                                                                 \n"," dense (Dense)               (None, 3)                 75267     \n","                                                                 \n","=================================================================\n","Total params: 14,789,955\n","Trainable params: 14,789,955\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":7,"id":"cb443b45","metadata":{"execution":{"iopub.execute_input":"2023-06-16T05:58:22.982017Z","iopub.status.busy":"2023-06-16T05:58:22.981739Z","iopub.status.idle":"2023-06-16T05:58:22.99917Z","shell.execute_reply":"2023-06-16T05:58:22.998301Z"},"papermill":{"duration":0.031137,"end_time":"2023-06-16T05:58:23.001146","exception":false,"start_time":"2023-06-16T05:58:22.970009","status":"completed"},"tags":[]},"outputs":[],"source":["#Categorical cross-entropy is commonly used as the loss function for multi-class classification problems, \n","#where each input can belong to only one class. It measures the dissimilarity between the predicted class probabilities and the true class labels.\n","\n","model.compile(\n","    loss = 'categorical_crossentropy',\n","    optimizer = 'adam',\n","    metrics=['accuracy']\n",")"]},{"cell_type":"code","execution_count":8,"id":"8923cd05","metadata":{"execution":{"iopub.execute_input":"2023-06-16T05:58:23.024154Z","iopub.status.busy":"2023-06-16T05:58:23.023895Z","iopub.status.idle":"2023-06-16T05:58:24.59649Z","shell.execute_reply":"2023-06-16T05:58:24.595503Z"},"papermill":{"duration":1.587043,"end_time":"2023-06-16T05:58:24.599158","exception":false,"start_time":"2023-06-16T05:58:23.012115","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 1722 images belonging to 3 classes.\n","Found 215 images belonging to 3 classes.\n"]}],"source":["from keras.preprocessing.image import ImageDataGenerator\n","train_datagen = ImageDataGenerator(rescale = 1./255,\n","                                   shear_range = 0.2,\n","                                   zoom_range = 0.2,\n","                                   horizontal_flip = True\n",")\n","\n","test_datagen = ImageDataGenerator(rescale = 1./255)\n","\n","training_set = train_datagen.flow_from_directory('/kaggle/input/potato-disease/train',\n","                                                 target_size = (224,224),\n","                                                 batch_size = 32,\n","                                                 class_mode = 'categorical'\n","                                                 )\n","\n","test_set = test_datagen.flow_from_directory('/kaggle/input/potato-disease/test',\n","                                            target_size = (224,224),\n","                                            batch_size = 32,\n","                                            class_mode='categorical'\n","                                            )"]},{"cell_type":"code","execution_count":9,"id":"c21e0a96","metadata":{"execution":{"iopub.execute_input":"2023-06-16T05:58:24.623902Z","iopub.status.busy":"2023-06-16T05:58:24.623143Z","iopub.status.idle":"2023-06-16T06:29:50.294165Z","shell.execute_reply":"2023-06-16T06:29:50.293149Z"},"papermill":{"duration":1885.686014,"end_time":"2023-06-16T06:29:50.296833","exception":false,"start_time":"2023-06-16T05:58:24.610819","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_23/1167563505.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  r = model.fit_generator(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","54/54 [==============================] - 78s 896ms/step - loss: 2.7722 - accuracy: 0.4803 - val_loss: 0.8764 - val_accuracy: 0.4791\n","Epoch 2/50\n","54/54 [==============================] - 29s 529ms/step - loss: 0.7993 - accuracy: 0.6516 - val_loss: 0.5290 - val_accuracy: 0.8140\n","Epoch 3/50\n","54/54 [==============================] - 30s 549ms/step - loss: 0.5938 - accuracy: 0.7863 - val_loss: 0.4959 - val_accuracy: 0.8093\n","Epoch 4/50\n","54/54 [==============================] - 30s 544ms/step - loss: 0.4173 - accuracy: 0.8473 - val_loss: 0.4171 - val_accuracy: 0.8512\n","Epoch 5/50\n","54/54 [==============================] - 30s 546ms/step - loss: 0.3894 - accuracy: 0.8583 - val_loss: 0.3411 - val_accuracy: 0.8698\n","Epoch 6/50\n","54/54 [==============================] - 30s 553ms/step - loss: 0.2561 - accuracy: 0.9042 - val_loss: 0.8552 - val_accuracy: 0.7535\n","Epoch 7/50\n","54/54 [==============================] - 30s 544ms/step - loss: 0.2536 - accuracy: 0.9129 - val_loss: 0.3786 - val_accuracy: 0.8465\n","Epoch 8/50\n","54/54 [==============================] - 30s 549ms/step - loss: 0.1521 - accuracy: 0.9448 - val_loss: 0.1643 - val_accuracy: 0.9488\n","Epoch 9/50\n","54/54 [==============================] - 29s 536ms/step - loss: 0.1482 - accuracy: 0.9413 - val_loss: 0.2808 - val_accuracy: 0.8930\n","Epoch 10/50\n","54/54 [==============================] - 30s 548ms/step - loss: 0.1597 - accuracy: 0.9460 - val_loss: 0.0870 - val_accuracy: 0.9674\n","Epoch 11/50\n","54/54 [==============================] - 30s 552ms/step - loss: 0.0907 - accuracy: 0.9657 - val_loss: 0.1646 - val_accuracy: 0.9349\n","Epoch 12/50\n","54/54 [==============================] - 30s 551ms/step - loss: 0.0742 - accuracy: 0.9733 - val_loss: 0.1083 - val_accuracy: 0.9535\n","Epoch 13/50\n","54/54 [==============================] - 30s 543ms/step - loss: 0.0814 - accuracy: 0.9704 - val_loss: 0.1893 - val_accuracy: 0.9163\n","Epoch 14/50\n","54/54 [==============================] - 32s 598ms/step - loss: 0.1248 - accuracy: 0.9553 - val_loss: 0.1251 - val_accuracy: 0.9488\n","Epoch 15/50\n","54/54 [==============================] - 31s 574ms/step - loss: 0.0575 - accuracy: 0.9756 - val_loss: 0.0423 - val_accuracy: 0.9767\n","Epoch 16/50\n","54/54 [==============================] - 33s 596ms/step - loss: 0.0595 - accuracy: 0.9774 - val_loss: 0.0575 - val_accuracy: 0.9814\n","Epoch 17/50\n","54/54 [==============================] - 32s 583ms/step - loss: 0.0478 - accuracy: 0.9866 - val_loss: 0.0568 - val_accuracy: 0.9674\n","Epoch 18/50\n","54/54 [==============================] - 32s 582ms/step - loss: 0.0514 - accuracy: 0.9791 - val_loss: 0.0327 - val_accuracy: 0.9814\n","Epoch 19/50\n","54/54 [==============================] - 31s 574ms/step - loss: 0.1668 - accuracy: 0.9408 - val_loss: 0.2949 - val_accuracy: 0.8698\n","Epoch 20/50\n","54/54 [==============================] - 31s 563ms/step - loss: 0.0894 - accuracy: 0.9710 - val_loss: 0.4723 - val_accuracy: 0.8605\n","Epoch 21/50\n","54/54 [==============================] - 31s 572ms/step - loss: 0.0906 - accuracy: 0.9681 - val_loss: 0.0944 - val_accuracy: 0.9581\n","Epoch 22/50\n","54/54 [==============================] - 31s 564ms/step - loss: 0.0478 - accuracy: 0.9803 - val_loss: 0.0982 - val_accuracy: 0.9628\n","Epoch 23/50\n","54/54 [==============================] - 31s 561ms/step - loss: 0.0530 - accuracy: 0.9779 - val_loss: 0.0557 - val_accuracy: 0.9907\n","Epoch 24/50\n","54/54 [==============================] - 31s 563ms/step - loss: 0.0322 - accuracy: 0.9855 - val_loss: 0.0213 - val_accuracy: 0.9953\n","Epoch 25/50\n","54/54 [==============================] - 31s 564ms/step - loss: 0.0428 - accuracy: 0.9837 - val_loss: 0.1215 - val_accuracy: 0.9581\n","Epoch 26/50\n","54/54 [==============================] - 32s 579ms/step - loss: 0.0444 - accuracy: 0.9861 - val_loss: 0.0428 - val_accuracy: 0.9814\n","Epoch 27/50\n","54/54 [==============================] - 32s 589ms/step - loss: 0.0902 - accuracy: 0.9669 - val_loss: 0.1924 - val_accuracy: 0.9302\n","Epoch 28/50\n","54/54 [==============================] - 32s 590ms/step - loss: 0.1123 - accuracy: 0.9704 - val_loss: 0.0342 - val_accuracy: 0.9907\n","Epoch 29/50\n","54/54 [==============================] - 32s 589ms/step - loss: 0.1170 - accuracy: 0.9640 - val_loss: 0.0639 - val_accuracy: 0.9674\n","Epoch 30/50\n","54/54 [==============================] - 32s 583ms/step - loss: 0.1507 - accuracy: 0.9448 - val_loss: 0.0918 - val_accuracy: 0.9581\n","Epoch 31/50\n","54/54 [==============================] - 31s 577ms/step - loss: 0.0392 - accuracy: 0.9837 - val_loss: 0.1314 - val_accuracy: 0.9628\n","Epoch 32/50\n","54/54 [==============================] - 31s 578ms/step - loss: 0.0485 - accuracy: 0.9837 - val_loss: 0.0253 - val_accuracy: 0.9907\n","Epoch 33/50\n","54/54 [==============================] - 31s 577ms/step - loss: 0.0256 - accuracy: 0.9907 - val_loss: 0.0089 - val_accuracy: 1.0000\n","Epoch 34/50\n","54/54 [==============================] - 31s 572ms/step - loss: 0.0592 - accuracy: 0.9785 - val_loss: 0.1909 - val_accuracy: 0.9256\n","Epoch 35/50\n","54/54 [==============================] - 31s 576ms/step - loss: 0.0320 - accuracy: 0.9872 - val_loss: 0.0581 - val_accuracy: 0.9721\n","Epoch 36/50\n","54/54 [==============================] - 31s 576ms/step - loss: 0.0213 - accuracy: 0.9925 - val_loss: 0.0913 - val_accuracy: 0.9628\n","Epoch 37/50\n","54/54 [==============================] - 31s 570ms/step - loss: 0.0110 - accuracy: 0.9948 - val_loss: 0.0166 - val_accuracy: 0.9907\n","Epoch 38/50\n","54/54 [==============================] - 31s 574ms/step - loss: 0.0968 - accuracy: 0.9663 - val_loss: 0.1008 - val_accuracy: 0.9628\n","Epoch 39/50\n","54/54 [==============================] - 31s 574ms/step - loss: 0.0492 - accuracy: 0.9797 - val_loss: 0.0176 - val_accuracy: 0.9953\n","Epoch 40/50\n","54/54 [==============================] - 31s 570ms/step - loss: 0.0462 - accuracy: 0.9843 - val_loss: 0.0512 - val_accuracy: 0.9860\n","Epoch 41/50\n","54/54 [==============================] - 31s 566ms/step - loss: 0.0273 - accuracy: 0.9895 - val_loss: 0.0762 - val_accuracy: 0.9767\n","Epoch 42/50\n","54/54 [==============================] - 31s 575ms/step - loss: 0.0142 - accuracy: 0.9954 - val_loss: 0.1075 - val_accuracy: 0.9628\n","Epoch 43/50\n","54/54 [==============================] - 31s 566ms/step - loss: 0.0411 - accuracy: 0.9826 - val_loss: 0.1402 - val_accuracy: 0.9535\n","Epoch 44/50\n","54/54 [==============================] - 30s 553ms/step - loss: 0.0562 - accuracy: 0.9797 - val_loss: 0.0744 - val_accuracy: 0.9721\n","Epoch 45/50\n","54/54 [==============================] - 30s 550ms/step - loss: 0.0797 - accuracy: 0.9715 - val_loss: 0.0355 - val_accuracy: 0.9907\n","Epoch 46/50\n","54/54 [==============================] - 30s 547ms/step - loss: 0.0612 - accuracy: 0.9779 - val_loss: 0.0419 - val_accuracy: 0.9814\n","Epoch 47/50\n","54/54 [==============================] - 30s 544ms/step - loss: 0.0415 - accuracy: 0.9855 - val_loss: 0.0228 - val_accuracy: 0.9953\n","Epoch 48/50\n","54/54 [==============================] - 29s 539ms/step - loss: 0.0276 - accuracy: 0.9913 - val_loss: 0.0119 - val_accuracy: 0.9953\n","Epoch 49/50\n","54/54 [==============================] - 31s 558ms/step - loss: 0.0152 - accuracy: 0.9942 - val_loss: 0.0237 - val_accuracy: 0.9860\n","Epoch 50/50\n","54/54 [==============================] - 31s 563ms/step - loss: 0.0179 - accuracy: 0.9913 - val_loss: 0.1165 - val_accuracy: 0.9628\n"]}],"source":["r = model.fit_generator(\n","  training_set,\n","  validation_data=test_set,\n","  epochs=50,\n","  steps_per_epoch=len(training_set),\n","  validation_steps=len(test_set)\n",")"]},{"cell_type":"code","execution_count":null,"id":"e33d0d4e","metadata":{"papermill":{"duration":0.232928,"end_time":"2023-06-16T06:29:50.753029","exception":false,"start_time":"2023-06-16T06:29:50.520101","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"edbe7f3c","metadata":{"papermill":{"duration":0.219767,"end_time":"2023-06-16T06:29:51.202647","exception":false,"start_time":"2023-06-16T06:29:50.98288","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"papermill":{"default_parameters":{},"duration":1928.747829,"end_time":"2023-06-16T06:29:55.331626","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-06-16T05:57:46.583797","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}